# Containerization

Step one in the project was to containerize the application. This involved creating Docker images for each of the services.

## Introduction

Containerization is a powerful technology that allows us to package applications and their dependencies into a single, portable unit called a container. This approach simplifies deployment, enhances scalability, and improves consistency across different environments.

![Container Architecture](./resources/1.1.container-architecture.png)

### Objective

With minimum code change and configuration, I aimed to containerize the entire project, which consists of four services. This allows for easier deployment and management of the application across various environments.

## Process

I started by analyzing the existing codebase and identifying the dependencies for each service.

### Frontend

The frontend service was built using React. Looking at the `package.json` file, I identified scripts for building, testing, and starting the application. Because this service is built on top of **Node.js**, dependency management was straightforward with **npm**.

I examined testing script and found that it uses **Jest** but project does not have any test cases outside of **cypress** tests which are UI tests. Script was returning `exit code 1` which means that it was not running any tests. I updated the script to pass when no tests found because we don't want CI to halt if no tests defined. I decided to keep testing stage in the Dockerfile for future enhancement when test cases will be added to the project.

Frontend services need web servers to serve static files, so I used **Nginx** as the web server for this service. I wrote a basic configuration under `server/nginx.conf` to serve the static files generated by the React build process. The configuration handles:
- Gzip compression for faster load times
- Caching for static assets to improve performance
- Routing to ensure that all requests are directed to the `index.html` file, which is essential for single-page applications (SPAs) like React.
- Health check endpoint to monitor the health of the service.

> **Note**: Addresses for backend services were set to `localhost` in the frontend code for local development. I updated these addresses to use relative paths (e.g., `/api/record`) to ensure that the frontend can communicate with the backend services when running in a containerized environment. This change allows the frontend to work seamlessly both in local development and in production environments without requiring code changes.

#### Dockerfile

Multi-stage builds were used to optimize the Dockerfile and reduce the final image size. Stages included:
1. **Base Stage**: Uses official Node.js Alpine image to set the base image and environment.
2. **Install Stage**: Installs dependencies via `npm clean-install` to ensure a clean installation of packages.
3. **Build Stage**: Copies dependencies from **installer** and builds the React application using `npm run build`.
4. **Test Stage**: Runs tests using `npm test` to ensure that the application is functioning correctly. Will be used in CI pipeline with explicit image targeting.
5. **Run Stage**: Uses the official Nginx Alpine image to serve the built application. Copies the built files from the build stage and the Nginx configuration file.

#### Enhancements

- Used multi-stage builds to optimize the image size and improve build times.
- Used slim or Alpine base images to further reduce the image size.
- Each stage is designed to perform specific tasks, which helps in keeping the final image clean and efficient.
- Health check is implemented to ensure that the service is running properly and can be monitored effectively.

### Backend

The backend service was built using **Node.js** and **Express**. Similar to the frontend, I analyzed the `package.json` file to identify the scripts for building, testing, and starting the application. The backend service also uses **npm** for dependency management.

I again examined testing script and found that it was returning `exit code 1` for the same reason as frontend. I updated the script to pass when no tests found because we don't want CI to halt if no tests defined. I decided to keep testing stage in the Dockerfile for future enhancement when test cases will be added to the project.

> **Note**: Project should use Node.js's built-in `--env-file={path}` flag to load environment variables from a file instead of using **dotenv** package. This reduce the number of dependencies and also allows us to manage environment variables more securely in production environments.

> **Note**: Project should use a source folder to keep source code and configuration files separate. This will help in better organization and easier `.*ignore` files management.

#### Dockerfile

For the backend service, I used the official Node.js Alpine image as the base image. Because we don't need a web server, we can use the Node.js image directly to run the application. The Dockerfile for the backend service includes:
1. **Base Stage**: Uses the official Node.js Alpine image to set the base image and environment.
2. **Install Stage**: Installs dependencies via `npm clean-install` to ensure a clean installation of packages.
3. **Test Stage**: Runs tests using `npm test` to ensure that the application is functioning correctly. Will be used in CI pipeline with explicit image targeting.
4. **Run Stage**: Copies the application files and starts the server using `node server.mjs`.

#### Enhancements

- Other than mentioned optimizations, I also implemented non-root user to run the application for better security.

### Database

For local development, I used **MongoDB** image to containerize the database. In code, developers were using `localhost` to connect to the database, which is not necessary when using Docker Compose as the local development environment. I didn't change the source code but provided the environment variable `ATLAS_URI` in the `compose.yaml`.

The data for the database is stored in a Docker volume to ensure that it persists across container restarts and is not lost when the container is removed.

### Python Service

The Python service is a simple script which requested to run every hour. I examined the code and found that it uses **requests** library to make HTTP requests. I created a requirements file to manage the dependencies.

The real question I had was should I use crontab in the image to run the script every hour or should I let orchestrator handle the scheduling. I decided to let orchestrator handle the scheduling because it is more efficient and easier to manage.

> **Note**: For local development, you can use tools like **cron** or **schedule** in Python to run the script every hour.

#### Dockerfile

I created a Dockerfile for the Python service which includes:
1. **Base Stage**: Uses the official Python Alpine image to set the base image and environment.
2. **Install Stage**: Installs dependencies using `pip install -r requirements.txt` to ensure that all necessary packages are installed.
3. **Test Stage**: Currently does nothing and prints "No tests defined." This can be enhanced in the future when test cases are added to the project.
4. **Run Stage**: Uses a simple command to run the script. The scheduling will be handled by the orchestrator (e.g., Kubernetes CronJob or Docker Compose with a scheduled task).

#### Enhancements

- Python and Pip require some configuration when running in a CI environment. The base stage is the perfect place to set these configurations.
